{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\janni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from own.loading import load_reviews_and_rids\n",
    "from own.loading import load_train_test_rid_lists\n",
    "from own.loading import load_RID_and_rating\n",
    "\n",
    "from own.functions import get_matching_reviews\n",
    "\n",
    "from own.vocab import load_vocab\n",
    "\n",
    "from own.classification_preparation import reviews_to_string\n",
    "\n",
    "from own.classification_pretrained import encode_and_pad_seqs\n",
    "from own.classification_pretrained import create_embedding_layer\n",
    "from own.classification_pretrained import create_model\n",
    "from own.classification_pretrained import calc_metrics\n",
    "from own.classification_pretrained import load_embedding\n",
    "from own.classification_pretrained import get_weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden und Vorbereiten von Train- und Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(os.path.join(\"data\",\"vocabs\",\"train_vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "File loaded successfully\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(\"data\", \"reviews\")\n",
    "train_path = os.path.join(directory, \"processed_trainset.txt\")\n",
    "test_path = os.path.join(directory, \"processed_testset.txt\")\n",
    "\n",
    "texts_trainset, rids_trainset = load_reviews_and_rids(train_path)\n",
    "texts_testset, rids_testset = load_reviews_and_rids(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = reviews_to_string(texts_trainset)\n",
    "test_docs = reviews_to_string(texts_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Found 1800 of 1800 seached results\n",
      "Found 200 of 200 seached results\n"
     ]
    }
   ],
   "source": [
    "df_rating = load_RID_and_rating()\n",
    "rid_values = np.array(df_rating.RID.values)\n",
    "rating_values = np.array(df_rating.rating.values)\n",
    "\n",
    "ytrain, train_matching_RIDs = np.array(get_matching_reviews(rid_values, rating_values, rids_trainset))\n",
    "ytest, test_matching_RIDs = np.array(get_matching_reviews(rid_values, rating_values, rids_testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_index, Xtrain, Xtest, vocab_size, max_length = encode_and_pad_seqs(train_docs, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"models\", \"word2vec_embeddings_cbow.txt\")\n",
    "embedding_layer = create_embedding_layer(file_path, vocab_size, max_length,tokenizer_index, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 804, 100)          1301100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 800, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51201     \n",
      "=================================================================\n",
      "Total params: 1,416,429\n",
      "Trainable params: 115,329\n",
      "Non-trainable params: 1,301,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "cbow_model = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples\n",
      "Epoch 1/15\n",
      "1800/1800 - 6s - loss: 0.2942 - true_positives: 1639.0000 - true_negatives: 2.0000 - false_positives: 140.0000 - false_negatives: 19.0000\n",
      "Epoch 2/15\n",
      "1800/1800 - 5s - loss: 0.2627 - true_positives: 1658.0000 - true_negatives: 0.0000e+00 - false_positives: 142.0000 - false_negatives: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1800/1800 - 4s - loss: 0.2476 - true_positives: 1658.0000 - true_negatives: 0.0000e+00 - false_positives: 142.0000 - false_negatives: 0.0000e+00\n",
      "Epoch 4/15\n",
      "1800/1800 - 5s - loss: 0.2191 - true_positives: 1658.0000 - true_negatives: 3.0000 - false_positives: 139.0000 - false_negatives: 0.0000e+00\n",
      "Epoch 5/15\n",
      "1800/1800 - 5s - loss: 0.1917 - true_positives: 1656.0000 - true_negatives: 12.0000 - false_positives: 130.0000 - false_negatives: 2.0000\n",
      "Epoch 6/15\n",
      "1800/1800 - 5s - loss: 0.1731 - true_positives: 1649.0000 - true_negatives: 28.0000 - false_positives: 114.0000 - false_negatives: 9.0000\n",
      "Epoch 7/15\n",
      "1800/1800 - 5s - loss: 0.1551 - true_positives: 1650.0000 - true_negatives: 44.0000 - false_positives: 98.0000 - false_negatives: 8.0000\n",
      "Epoch 8/15\n",
      "1800/1800 - 5s - loss: 0.1383 - true_positives: 1654.0000 - true_negatives: 55.0000 - false_positives: 87.0000 - false_negatives: 4.0000\n",
      "Epoch 9/15\n",
      "1800/1800 - 5s - loss: 0.1208 - true_positives: 1652.0000 - true_negatives: 67.0000 - false_positives: 75.0000 - false_negatives: 6.0000\n",
      "Epoch 10/15\n",
      "1800/1800 - 5s - loss: 0.1101 - true_positives: 1652.0000 - true_negatives: 76.0000 - false_positives: 66.0000 - false_negatives: 6.0000\n",
      "Epoch 11/15\n",
      "1800/1800 - 4s - loss: 0.0992 - true_positives: 1653.0000 - true_negatives: 88.0000 - false_positives: 54.0000 - false_negatives: 5.0000\n",
      "Epoch 12/15\n",
      "1800/1800 - 4s - loss: 0.0908 - true_positives: 1652.0000 - true_negatives: 91.0000 - false_positives: 51.0000 - false_negatives: 6.0000\n",
      "Epoch 13/15\n",
      "1800/1800 - 4s - loss: 0.0771 - true_positives: 1656.0000 - true_negatives: 103.0000 - false_positives: 39.0000 - false_negatives: 2.0000\n",
      "Epoch 14/15\n",
      "1800/1800 - 4s - loss: 0.0708 - true_positives: 1656.0000 - true_negatives: 104.0000 - false_positives: 38.0000 - false_negatives: 2.0000\n",
      "Epoch 15/15\n",
      "1800/1800 - 5s - loss: 0.0653 - true_positives: 1654.0000 - true_negatives: 107.0000 - false_positives: 35.0000 - false_negatives: 4.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x279511ae808>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "cbow_model.fit(Xtrain, ytrain, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(\"data\",\"models\",\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save(os.path.join(dir,'w2v_cbow_classifier.h5'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model('w2v_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_test_loss, cbow_test_tp, cbow_test_tn, cbow_test_fp, cbow_test_fn = cbow_model.evaluate(Xtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_test\n",
      " Precision: 0.9513513445854187\n",
      " Recall: 0.9513513445854187\n",
      " Negative Prediction Value: 0.4000000059604645\n",
      " Specificity: 0.4000000059604645\n",
      " Error Rate: 0.09000000357627869\n",
      " F1-Score: 0.9513513134211226\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"cbow_test\", cbow_test_tp, cbow_test_tn, cbow_test_fp, cbow_test_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"models\", \"word2vec_embeddings_skip_model.txt\")\n",
    "embedding_layer = create_embedding_layer(file_path, vocab_size, max_length,tokenizer_index, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 804, 100)          1301100   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 800, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51201     \n",
      "=================================================================\n",
      "Total params: 1,416,429\n",
      "Trainable params: 115,329\n",
      "Non-trainable params: 1,301,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "skip_model = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples\n",
      "Epoch 1/15\n",
      "1800/1800 - 5s - loss: 0.2975 - true_positives_1: 1652.0000 - true_negatives_1: 1.0000 - false_positives_1: 141.0000 - false_negatives_1: 6.0000\n",
      "Epoch 2/15\n",
      "1800/1800 - 5s - loss: 0.2478 - true_positives_1: 1658.0000 - true_negatives_1: 0.0000e+00 - false_positives_1: 142.0000 - false_negatives_1: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1800/1800 - 4s - loss: 0.2300 - true_positives_1: 1658.0000 - true_negatives_1: 0.0000e+00 - false_positives_1: 142.0000 - false_negatives_1: 0.0000e+00\n",
      "Epoch 4/15\n",
      "1800/1800 - 4s - loss: 0.2053 - true_positives_1: 1658.0000 - true_negatives_1: 4.0000 - false_positives_1: 138.0000 - false_negatives_1: 0.0000e+00\n",
      "Epoch 5/15\n",
      "1800/1800 - 4s - loss: 0.1896 - true_positives_1: 1657.0000 - true_negatives_1: 10.0000 - false_positives_1: 132.0000 - false_negatives_1: 1.0000\n",
      "Epoch 6/15\n",
      "1800/1800 - 5s - loss: 0.1615 - true_positives_1: 1656.0000 - true_negatives_1: 29.0000 - false_positives_1: 113.0000 - false_negatives_1: 2.0000\n",
      "Epoch 7/15\n",
      "1800/1800 - 5s - loss: 0.1409 - true_positives_1: 1651.0000 - true_negatives_1: 40.0000 - false_positives_1: 102.0000 - false_negatives_1: 7.0000\n",
      "Epoch 8/15\n",
      "1800/1800 - 4s - loss: 0.1249 - true_positives_1: 1653.0000 - true_negatives_1: 61.0000 - false_positives_1: 81.0000 - false_negatives_1: 5.0000\n",
      "Epoch 9/15\n",
      "1800/1800 - 4s - loss: 0.1106 - true_positives_1: 1653.0000 - true_negatives_1: 69.0000 - false_positives_1: 73.0000 - false_negatives_1: 5.0000\n",
      "Epoch 10/15\n",
      "1800/1800 - 4s - loss: 0.0965 - true_positives_1: 1654.0000 - true_negatives_1: 81.0000 - false_positives_1: 61.0000 - false_negatives_1: 4.0000\n",
      "Epoch 11/15\n",
      "1800/1800 - 4s - loss: 0.0843 - true_positives_1: 1653.0000 - true_negatives_1: 94.0000 - false_positives_1: 48.0000 - false_negatives_1: 5.0000\n",
      "Epoch 12/15\n",
      "1800/1800 - 5s - loss: 0.0747 - true_positives_1: 1656.0000 - true_negatives_1: 99.0000 - false_positives_1: 43.0000 - false_negatives_1: 2.0000\n",
      "Epoch 13/15\n",
      "1800/1800 - 4s - loss: 0.0806 - true_positives_1: 1653.0000 - true_negatives_1: 96.0000 - false_positives_1: 46.0000 - false_negatives_1: 5.0000\n",
      "Epoch 14/15\n",
      "1800/1800 - 4s - loss: 0.0559 - true_positives_1: 1654.0000 - true_negatives_1: 109.0000 - false_positives_1: 33.0000 - false_negatives_1: 4.0000\n",
      "Epoch 15/15\n",
      "1800/1800 - 5s - loss: 0.0488 - true_positives_1: 1655.0000 - true_negatives_1: 119.0000 - false_positives_1: 23.0000 - false_negatives_1: 3.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27953f2b748>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "skip_model.fit(Xtrain, ytrain, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_train_loss, skip_train_tp, skip_train_tn, skip_train_fp, skip_train_fn = skip_model.evaluate(Xtrain, ytrain, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_gram_train\n",
      " Precision: 0.9863013625144958\n",
      " Recall: 0.9987937211990356\n",
      " Negative Prediction Value: 0.9834710955619812\n",
      " Specificity: 0.8380281925201416\n",
      " Error Rate: 0.013888888992369175\n",
      " F1-Score: 0.992508249968893\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"skip_gram_train\", skip_train_tp, skip_train_tn, skip_train_fp, skip_train_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_model.save(os.path.join(dir,'w2v_skip_model_classifier.h5'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model('w2v_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_test_loss, skip_test_tp, skip_test_tn, skip_test_fp, skip_test_fn = skip_model.evaluate(Xtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_gram_test\n",
      " Precision: 0.9292929172515869\n",
      " Recall: 0.9945945739746094\n",
      " Negative Prediction Value: 0.5\n",
      " Specificity: 0.06666667014360428\n",
      " Error Rate: 0.07500000298023224\n",
      " F1-Score: 0.9608354754876125\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"skip_gram_test\", skip_test_tp, skip_test_tn, skip_test_fp, skip_test_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Skip Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"models\", \"word2vec_embeddings_opt_skip_model.txt\")\n",
    "embedding_layer = create_embedding_layer(file_path, vocab_size, max_length,tokenizer_index, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 804, 100)          1301100   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 800, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51201     \n",
      "=================================================================\n",
      "Total params: 1,416,429\n",
      "Trainable params: 115,329\n",
      "Non-trainable params: 1,301,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "opt_skip_model = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples\n",
      "Epoch 1/15\n",
      "1800/1800 - 6s - loss: 0.3162 - true_positives_2: 1629.0000 - true_negatives_2: 1.0000 - false_positives_2: 141.0000 - false_negatives_2: 29.0000\n",
      "Epoch 2/15\n",
      "1800/1800 - 5s - loss: 0.2388 - true_positives_2: 1658.0000 - true_negatives_2: 0.0000e+00 - false_positives_2: 142.0000 - false_negatives_2: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1800/1800 - 4s - loss: 0.2050 - true_positives_2: 1658.0000 - true_negatives_2: 0.0000e+00 - false_positives_2: 142.0000 - false_negatives_2: 0.0000e+00\n",
      "Epoch 4/15\n",
      "1800/1800 - 5s - loss: 0.1721 - true_positives_2: 1658.0000 - true_negatives_2: 15.0000 - false_positives_2: 127.0000 - false_negatives_2: 0.0000e+00\n",
      "Epoch 5/15\n",
      "1800/1800 - 5s - loss: 0.1424 - true_positives_2: 1657.0000 - true_negatives_2: 38.0000 - false_positives_2: 104.0000 - false_negatives_2: 1.0000\n",
      "Epoch 6/15\n",
      "1800/1800 - 5s - loss: 0.1128 - true_positives_2: 1654.0000 - true_negatives_2: 61.0000 - false_positives_2: 81.0000 - false_negatives_2: 4.0000\n",
      "Epoch 7/15\n",
      "1800/1800 - 5s - loss: 0.0944 - true_positives_2: 1655.0000 - true_negatives_2: 84.0000 - false_positives_2: 58.0000 - false_negatives_2: 3.0000\n",
      "Epoch 8/15\n",
      "1800/1800 - 5s - loss: 0.0663 - true_positives_2: 1655.0000 - true_negatives_2: 103.0000 - false_positives_2: 39.0000 - false_negatives_2: 3.0000\n",
      "Epoch 9/15\n",
      "1800/1800 - 5s - loss: 0.0494 - true_positives_2: 1656.0000 - true_negatives_2: 114.0000 - false_positives_2: 28.0000 - false_negatives_2: 2.0000\n",
      "Epoch 10/15\n",
      "1800/1800 - 5s - loss: 0.0403 - true_positives_2: 1656.0000 - true_negatives_2: 128.0000 - false_positives_2: 14.0000 - false_negatives_2: 2.0000\n",
      "Epoch 11/15\n",
      "1800/1800 - 5s - loss: 0.0319 - true_positives_2: 1655.0000 - true_negatives_2: 128.0000 - false_positives_2: 14.0000 - false_negatives_2: 3.0000\n",
      "Epoch 12/15\n",
      "1800/1800 - 5s - loss: 0.0305 - true_positives_2: 1655.0000 - true_negatives_2: 134.0000 - false_positives_2: 8.0000 - false_negatives_2: 3.0000\n",
      "Epoch 13/15\n",
      "1800/1800 - 5s - loss: 0.0194 - true_positives_2: 1656.0000 - true_negatives_2: 137.0000 - false_positives_2: 5.0000 - false_negatives_2: 2.0000\n",
      "Epoch 14/15\n",
      "1800/1800 - 5s - loss: 0.0156 - true_positives_2: 1656.0000 - true_negatives_2: 138.0000 - false_positives_2: 4.0000 - false_negatives_2: 2.0000\n",
      "Epoch 15/15\n",
      "1800/1800 - 5s - loss: 0.0126 - true_positives_2: 1656.0000 - true_negatives_2: 138.0000 - false_positives_2: 4.0000 - false_negatives_2: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27957c32dc8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "opt_skip_model.fit(Xtrain, ytrain, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip_model.save(os.path.join(dir,'w2v_opt_skip_model_classifier.h5'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model('w2v_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip_test_loss, opt_skip_test_tp, opt_skip_test_tn, opt_skip_test_fp, opt_skip_test_fn = opt_skip_model.evaluate(Xtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_skip_gram_test\n",
      " Precision: 0.9292929172515869\n",
      " Recall: 0.9945945739746094\n",
      " Negative Prediction Value: 0.5\n",
      " Specificity: 0.06666667014360428\n",
      " Error Rate: 0.07500000298023224\n",
      " F1-Score: 0.9608354754876125\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"opt_skip_gram_test\", opt_skip_test_tp, opt_skip_test_tn, opt_skip_test_fp, opt_skip_test_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Skip Gram 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"models\", \"word2vec_embeddings_opt_skip_model2.txt\")\n",
    "embedding_layer = create_embedding_layer(file_path, vocab_size, max_length,tokenizer_index, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 804, 100)          1301100   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 800, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51201     \n",
      "=================================================================\n",
      "Total params: 1,416,429\n",
      "Trainable params: 115,329\n",
      "Non-trainable params: 1,301,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "opt_skip_model2 = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples\n",
      "Epoch 1/15\n",
      "1800/1800 - 6s - loss: 0.3134 - true_positives_3: 1642.0000 - true_negatives_3: 5.0000 - false_positives_3: 137.0000 - false_negatives_3: 16.0000\n",
      "Epoch 2/15\n",
      "1800/1800 - 5s - loss: 0.2423 - true_positives_3: 1658.0000 - true_negatives_3: 0.0000e+00 - false_positives_3: 142.0000 - false_negatives_3: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1800/1800 - 5s - loss: 0.2127 - true_positives_3: 1658.0000 - true_negatives_3: 1.0000 - false_positives_3: 141.0000 - false_negatives_3: 0.0000e+00\n",
      "Epoch 4/15\n",
      "1800/1800 - 5s - loss: 0.1805 - true_positives_3: 1658.0000 - true_negatives_3: 13.0000 - false_positives_3: 129.0000 - false_negatives_3: 0.0000e+00\n",
      "Epoch 5/15\n",
      "1800/1800 - 5s - loss: 0.1461 - true_positives_3: 1658.0000 - true_negatives_3: 27.0000 - false_positives_3: 115.0000 - false_negatives_3: 0.0000e+00\n",
      "Epoch 6/15\n",
      "1800/1800 - 5s - loss: 0.1216 - true_positives_3: 1655.0000 - true_negatives_3: 55.0000 - false_positives_3: 87.0000 - false_negatives_3: 3.0000\n",
      "Epoch 7/15\n",
      "1800/1800 - 5s - loss: 0.0936 - true_positives_3: 1655.0000 - true_negatives_3: 81.0000 - false_positives_3: 61.0000 - false_negatives_3: 3.0000\n",
      "Epoch 8/15\n",
      "1800/1800 - 5s - loss: 0.0719 - true_positives_3: 1654.0000 - true_negatives_3: 100.0000 - false_positives_3: 42.0000 - false_negatives_3: 4.0000\n",
      "Epoch 9/15\n",
      "1800/1800 - 5s - loss: 0.0526 - true_positives_3: 1656.0000 - true_negatives_3: 112.0000 - false_positives_3: 30.0000 - false_negatives_3: 2.0000\n",
      "Epoch 10/15\n",
      "1800/1800 - 5s - loss: 0.0386 - true_positives_3: 1656.0000 - true_negatives_3: 124.0000 - false_positives_3: 18.0000 - false_negatives_3: 2.0000\n",
      "Epoch 11/15\n",
      "1800/1800 - 5s - loss: 0.0308 - true_positives_3: 1655.0000 - true_negatives_3: 130.0000 - false_positives_3: 12.0000 - false_negatives_3: 3.0000\n",
      "Epoch 12/15\n",
      "1800/1800 - 5s - loss: 0.0238 - true_positives_3: 1656.0000 - true_negatives_3: 134.0000 - false_positives_3: 8.0000 - false_negatives_3: 2.0000\n",
      "Epoch 13/15\n",
      "1800/1800 - 5s - loss: 0.0401 - true_positives_3: 1652.0000 - true_negatives_3: 125.0000 - false_positives_3: 17.0000 - false_negatives_3: 6.0000\n",
      "Epoch 14/15\n",
      "1800/1800 - 5s - loss: 0.0338 - true_positives_3: 1655.0000 - true_negatives_3: 129.0000 - false_positives_3: 13.0000 - false_negatives_3: 3.0000\n",
      "Epoch 15/15\n",
      "1800/1800 - 5s - loss: 0.0175 - true_positives_3: 1656.0000 - true_negatives_3: 136.0000 - false_positives_3: 6.0000 - false_negatives_3: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2795a8ba888>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "opt_skip_model2.fit(Xtrain, ytrain, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip_model2.save(os.path.join(dir,'w2v_opt_skip_model2_classifier.h5'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = load_model('w2v_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip2_test_loss, opt_skip2_test_tp, opt_skip2_test_tn, opt_skip2_test_fp, opt_skip2_test_fn = opt_skip_model2.evaluate(Xtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_skip_gram_test2\n",
      " Precision: 0.929648220539093\n",
      " Recall: 1.0\n",
      " Negative Prediction Value: 1.0\n",
      " Specificity: 0.06666667014360428\n",
      " Error Rate: 0.07000000029802322\n",
      " F1-Score: 0.9635416853286903\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"opt_skip_gram_test2\", opt_skip2_test_tp, opt_skip2_test_tn, opt_skip2_test_fp, opt_skip2_test_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Skip Gram + Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"models\", \"word2vec_embeddings_opt_skip_model2.txt\")\n",
    "raw_embedding = load_embedding(file_path)\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer_index, vocab_size, vocab)\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 804, 100)          1301100   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 800, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51201     \n",
      "=================================================================\n",
      "Total params: 1,416,429\n",
      "Trainable params: 1,416,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "opt_skip_model_trainable = create_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples\n",
      "Epoch 1/15\n",
      "1800/1800 - 7s - loss: 0.3012 - true_positives_4: 1642.0000 - true_negatives_4: 1.0000 - false_positives_4: 141.0000 - false_negatives_4: 16.0000\n",
      "Epoch 2/15\n",
      "1800/1800 - 6s - loss: 0.2321 - true_positives_4: 1658.0000 - true_negatives_4: 0.0000e+00 - false_positives_4: 142.0000 - false_negatives_4: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1800/1800 - 6s - loss: 0.1666 - true_positives_4: 1656.0000 - true_negatives_4: 12.0000 - false_positives_4: 130.0000 - false_negatives_4: 2.0000\n",
      "Epoch 4/15\n",
      "1800/1800 - 6s - loss: 0.0960 - true_positives_4: 1654.0000 - true_negatives_4: 86.0000 - false_positives_4: 56.0000 - false_negatives_4: 4.0000\n",
      "Epoch 5/15\n",
      "1800/1800 - 6s - loss: 0.0566 - true_positives_4: 1650.0000 - true_negatives_4: 115.0000 - false_positives_4: 27.0000 - false_negatives_4: 8.0000\n",
      "Epoch 6/15\n",
      "1800/1800 - 6s - loss: 0.0390 - true_positives_4: 1653.0000 - true_negatives_4: 121.0000 - false_positives_4: 21.0000 - false_negatives_4: 5.0000\n",
      "Epoch 7/15\n",
      "1800/1800 - 6s - loss: 0.0194 - true_positives_4: 1656.0000 - true_negatives_4: 137.0000 - false_positives_4: 5.0000 - false_negatives_4: 2.0000\n",
      "Epoch 8/15\n",
      "1800/1800 - 6s - loss: 0.0104 - true_positives_4: 1655.0000 - true_negatives_4: 141.0000 - false_positives_4: 1.0000 - false_negatives_4: 3.0000\n",
      "Epoch 9/15\n",
      "1800/1800 - 6s - loss: 0.0069 - true_positives_4: 1656.0000 - true_negatives_4: 142.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 2.0000\n",
      "Epoch 10/15\n",
      "1800/1800 - 6s - loss: 0.0063 - true_positives_4: 1656.0000 - true_negatives_4: 141.0000 - false_positives_4: 1.0000 - false_negatives_4: 2.0000\n",
      "Epoch 11/15\n",
      "1800/1800 - 6s - loss: 0.0053 - true_positives_4: 1655.0000 - true_negatives_4: 142.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 3.0000\n",
      "Epoch 12/15\n",
      "1800/1800 - 6s - loss: 0.0044 - true_positives_4: 1657.0000 - true_negatives_4: 141.0000 - false_positives_4: 1.0000 - false_negatives_4: 1.0000\n",
      "Epoch 13/15\n",
      "1800/1800 - 7s - loss: 0.0037 - true_positives_4: 1657.0000 - true_negatives_4: 142.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 1.0000\n",
      "Epoch 14/15\n",
      "1800/1800 - 6s - loss: 0.0027 - true_positives_4: 1657.0000 - true_negatives_4: 142.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 1.0000\n",
      "Epoch 15/15\n",
      "1800/1800 - 6s - loss: 0.0025 - true_positives_4: 1657.0000 - true_negatives_4: 142.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2795f61a348>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "opt_skip_model_trainable.fit(Xtrain, ytrain, epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip_model_trainable.save(os.path.join(dir,'w2v_opt_skip_model_trainable_classifier.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_skip_test_loss, opt_skip_test_tp, opt_skip_test_tn, opt_skip_test_fp, opt_skip_test_fn = opt_skip_model_trainable.evaluate(Xtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_skip_gram_training_test\n",
      " Precision: 0.9336734414100647\n",
      " Recall: 0.9891892075538635\n",
      " Negative Prediction Value: 0.5\n",
      " Specificity: 0.13333334028720856\n",
      " Error Rate: 0.07500000298023224\n",
      " F1-Score: 0.9606299163782883\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\"opt_skip_gram_training_test\", opt_skip_test_tp, opt_skip_test_tn, opt_skip_test_fp, opt_skip_test_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
