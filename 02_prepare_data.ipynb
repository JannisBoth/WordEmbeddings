{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from own.loading import load_train_test_rid_lists\n",
    "from own.loading import load_reviews_and_rids\n",
    "\n",
    "from own.processing import process_set # process_set(RID_list, dir, replacement_patterns) - no return\n",
    "\n",
    "from own.functions import get_matching_reviews\n",
    "\n",
    "from own.vocab import create_vocab\n",
    "from own.vocab import define_min_occurrence\n",
    "from own.vocab import save_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die RIDs des Test- und Trainsets laden\n",
    "\n",
    "*load_train_test_rid_lists* liefert aus den vorher abgespeicherten Dateien mit Test- und Trainingsset zwei Listen, mit den zugehörigen RIDs zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Trainset successfully\n",
      "Loaded Testset successfully\n"
     ]
    }
   ],
   "source": [
    "train_rids, test_rids = load_train_test_rid_lists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Texte und RIDs von plain_reviews.txt laden\n",
    "\n",
    "*load_reviews_and_rids* Lädt die Texte eines Reviews als Liste von Sätze und die dazugehörigen RIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n"
     ]
    }
   ],
   "source": [
    "review_list, RID_list = load_reviews_and_rids(file_path = os.path.join(\"data\",\"reviews\",\"plain_reviews.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verarbeitung des Testsets\n",
    "\n",
    "*get_matching_reviews* sucht die passenden Review-Texte zu gesuchten RIDs.\n",
    "\n",
    "*process_set* durchläuft mit jedem Review eines Sets alle in der Bachelorarbeit beschriebenen Verarbeitungsschritte und speichert folglich das Set in einer einzelnen Datei ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 of 200 seached results\n"
     ]
    }
   ],
   "source": [
    "matching_reviews, matching_RIDs = get_matching_reviews(RID_list, review_list, test_rids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  data\\reviews  already exists\n",
      "Saving processed_testset.txt\n",
      "File saved successfully\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_set(\"processed_testset\", matching_RIDs, matching_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verarbeitung des Trainingssets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 of 1800 seached results\n"
     ]
    }
   ],
   "source": [
    "matching_reviews, matching_RIDs = get_matching_reviews(RID_list, review_list, train_rids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  data\\reviews  already exists\n",
      "Saving processed_trainset.txt\n",
      "File saved successfully\n",
      "Wall time: 13min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_set(\"processed_trainset\", matching_RIDs, matching_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wortschatz des Trainingssets erstellen\n",
    "\n",
    "*load_reviews_and_rids* Lädt in diesem Falle die verarbeiteten Texte und RIDs des Trainsets\n",
    "\n",
    "*create_vocab* erstellt hierbei eine gezählte Repräsentation aller Tokens in den vorhandenen texten.\n",
    "\n",
    "*define_min_occurence* Liefert eine Liste mit validen Tokens zurück, die über $x$-mal vorkommen (x=2).\n",
    "\n",
    "*save_vocab* Speichert den Wortschatz in einer Datei ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"data\", \"reviews\", \"processed_trainset.txt\")\n",
    "train_review_list, train_RID_list = load_reviews_and_rids(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining min_occurence\n",
      " Vocab length before truncating: 13046\n",
      " Vocab length after truncating: 8109\n",
      "Directory  data\\vocabs  successfully created \n",
      "File train_vocab saved successfully\n",
      "Wall time: 94.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create vocab for trainset\n",
    "train_vocab = create_vocab(train_review_list)\n",
    "train_tokens = define_min_occurrence(train_vocab)\n",
    "\n",
    "directory = os.path.join(\"data\", \"vocabs\")\n",
    "file_name = \"train_vocab\"\n",
    "save_vocab(directory, file_name, train_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
